{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing all the files","metadata":{}},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2022-04-10T10:01:50.257332Z","iopub.execute_input":"2022-04-10T10:01:50.257902Z","iopub.status.idle":"2022-04-10T10:01:59.312444Z","shell.execute_reply.started":"2022-04-10T10:01:50.257807Z","shell.execute_reply":"2022-04-10T10:01:59.311668Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2022-04-10T10:01:59.314847Z","iopub.execute_input":"2022-04-10T10:01:59.315261Z","iopub.status.idle":"2022-04-10T10:02:00.897244Z","shell.execute_reply.started":"2022-04-10T10:01:59.315225Z","shell.execute_reply":"2022-04-10T10:02:00.896503Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport json\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom PIL import Image, ImageFilter\nfrom torchvision.transforms import ToPILImage\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nimport torchvision\nimport math\nimport torch.backends.cudnn as cudnn\nimport torch\nfrom torch import nn\nfrom torchvision.utils import make_grid\nfrom torch.utils.tensorboard import SummaryWriter\nimport os\nfrom math import log10","metadata":{"execution":{"iopub.status.busy":"2022-04-10T10:02:00.898543Z","iopub.execute_input":"2022-04-10T10:02:00.899216Z","iopub.status.idle":"2022-04-10T10:02:01.496167Z","shell.execute_reply.started":"2022-04-10T10:02:00.899179Z","shell.execute_reply":"2022-04-10T10:02:01.495444Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Defining the dataset class\n","metadata":{}},{"cell_type":"code","source":"class SRDataset(Dataset):\n    def __init__(self, data_path, crop_size, scaling_factor):\n        self.data_path=data_path\n        self.crop_size = int(crop_size)\n        self.scaling_factor = int(scaling_factor)\n        self.images_path=[]\n\n        \n        for name in os.listdir(self.data_path):\n            self.images_path.append(os.path.join(self.data_path,name))\n\n        # transformation common for both\n        self.pre_trans=transforms.Compose([\n                                transforms.CenterCrop(self.crop_size),\n                                transforms.RandomHorizontalFlip(p=0.5),\n                                transforms.RandomVerticalFlip(p=0.5)\n                                ])\n        \n        # transformation only for the input class [lr images]\n        self.input_transform = transforms.Compose([\n                                transforms.Resize(self.crop_size//self.scaling_factor),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean=[0.5],std=[0.5])\n                                ])\n        \n        # transformation only for the target class [hr images]\n        self.target_transform = transforms.Compose([\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean=[0.5],std=[0.5]),\n                                ])\n\n    # get an element based on index value\n    # get the lr image and the hr image\n    def __getitem__(self, i):\n        img = Image.open(self.images_path[i], mode='r')\n        img = img.convert('RGB')\n        img=self.pre_trans(img)\n\n        lr_img = self.input_transform(img)\n        hr_img = self.target_transform(img.copy())\n        \n        return lr_img, hr_img\n\n    # get the length of the input image\n    def __len__(self):\n        return len(self.images_path)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T10:02:01.498135Z","iopub.execute_input":"2022-04-10T10:02:01.498481Z","iopub.status.idle":"2022-04-10T10:02:01.509608Z","shell.execute_reply.started":"2022-04-10T10:02:01.498442Z","shell.execute_reply":"2022-04-10T10:02:01.508810Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Model definition","metadata":{}},{"cell_type":"code","source":"# defining the convolutional block\nclass ConvolutionalBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, batch_norm=False, activation=None):\n        super(ConvolutionalBlock, self).__init__()\n        # check if the activation is there or not\n        if activation is not None:\n            activation = activation.lower()\n            assert activation in {'prelu', 'leakyrelu', 'tanh'}\n        # initialize an empty list, which will store the layers of the model\n        layers = list()\n        # append the first layer of Conv\n        layers.append(\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n                      padding=kernel_size // 2))\n        # add the BatchNorm\n        if batch_norm is True:\n            layers.append(nn.BatchNorm2d(num_features=out_channels))\n        # batchNorm is followed by an activation layer\n        if activation == 'prelu':\n            layers.append(nn.PReLU())\n        elif activation == 'leakyrelu':\n            layers.append(nn.LeakyReLU(0.2))\n        elif activation == 'tanh':\n            layers.append(nn.Tanh())\n        # create a sequential model from the list of layers.\n        self.conv_block = nn.Sequential(*layers)\n    \n    # define the forward function\n    def forward(self, input):\n        output = self.conv_block(input)\n        return output\n\n\n# SubPixel Convolution layer\nclass SubPixelConvolutionalBlock(nn.Module):\n    # pixel shuffle layer is introduced to increase the dimension at the end.\n    def __init__(self, kernel_size=3, n_channels=64, scaling_factor=2):\n        super(SubPixelConvolutionalBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels=n_channels, out_channels=n_channels * (scaling_factor ** 2),\n                              kernel_size=kernel_size, padding=kernel_size // 2)\n        # r2*c*h*w --> r*c*rh*rw\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=scaling_factor)\n        self.prelu = nn.PReLU()\n\n    # define the forward function\n    def forward(self, input):\n        output = self.conv(input)\n        output = self.pixel_shuffle(output)  \n        output = self.prelu(output) \n\n        return output\n\n\n# define the residual block\nclass ResidualBlock(nn.Module):\n    def __init__(self, kernel_size=3, n_channels=64):\n        super(ResidualBlock, self).__init__()\n        self.conv_block1 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n                                              batch_norm=False, activation='PReLu')\n\n        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n                                              batch_norm=False, activation=None)\n\n    def forward(self, input):\n        residual = input  # (N, n_channels, w, h)\n        output = self.conv_block1(input)  # (N, n_channels, w, h)\n        output = self.conv_block2(output)  # (N, n_channels, w, h)\n        output = output + residual  # (N, n_channels, w, h)\n\n        return output\n\n\nclass SRResNet(nn.Module):\n    # defining the entire structure of the SRResNet\n    def __init__(self, large_kernel_size=9, small_kernel_size=3, n_channels=64, n_blocks=16, scaling_factor=4):\n        super(SRResNet, self).__init__()\n        scaling_factor = int(scaling_factor)\n        assert scaling_factor in {2, 4, 8}\n\n        self.conv_block1 = ConvolutionalBlock(in_channels=3, out_channels=n_channels, kernel_size=large_kernel_size,\n                                              batch_norm=False, activation='PReLu')\n\n        self.residual_blocks = nn.Sequential(\n            *[ResidualBlock(kernel_size=small_kernel_size, n_channels=n_channels) for i in range(n_blocks)])\n\n        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels,\n                                              kernel_size=small_kernel_size,\n                                              batch_norm=True, activation=None)\n\n        n_subpixel_convolution_blocks = int(math.log2(scaling_factor))\n        self.subpixel_convolutional_blocks = nn.Sequential(\n            *[SubPixelConvolutionalBlock(kernel_size=small_kernel_size, n_channels=64, scaling_factor=4) for i\n              in range(n_subpixel_convolution_blocks)])\n\n        self.conv_block3 = ConvolutionalBlock(in_channels=n_channels, out_channels=3, kernel_size=large_kernel_size,\n                                              batch_norm=False, activation='Tanh')\n\n    def forward(self, lr_imgs):\n        output = self.conv_block1(lr_imgs)  # (16, 3, 24, 24)\n        residual = output  # (16, 64, 24, 24)\n        output = self.residual_blocks(output)  # (16, 64, 24, 24)\n        output = self.conv_block2(output)  # (16, 64, 24, 24)\n        output = output + residual  # (16, 64, 24, 24)\n        output = self.subpixel_convolutional_blocks(output)  # (16, 64, 24 * 2, 24 * 2)\n        sr_imgs = self.conv_block3(output)  # (16, 3, 24 * 2, 24 * 2)\n\n        return sr_imgs\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:14:45.053739Z","iopub.execute_input":"2022-04-10T12:14:45.054040Z","iopub.status.idle":"2022-04-10T12:14:45.077387Z","shell.execute_reply.started":"2022-04-10T12:14:45.054009Z","shell.execute_reply":"2022-04-10T12:14:45.076611Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_path='../input/div2k/DIV2K_train_HR/DIV2K_train_HR'\ntest_path='../input/div2k/DIV2K_valid_HR/DIV2K_valid_HR'\n\ncrop_size = 600      \nscaling_factor = 2 \n\n\nlarge_kernel_size = 9   \nsmall_kernel_size = 3   \nn_channels = 64         \nn_blocks = 16           \n\n\ncheckpoint = './srresnet.pth'  \nbatch_size = 2    \nstart_epoch = 1     \nepochs = 20      \nworkers = 1        \nlr = 1e-4           \n\npre_psnr=0\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nngpu = 1\n\ncudnn.benchmark = True \n\n\nif os.path.exists(checkpoint):\n    model = torch.load(checkpoint).to(device)\n    print('Loading of previous model succeded')\nelse:\n    print('Original model not loaded, re-instantiating the model')\n    model = SRResNet(large_kernel_size=large_kernel_size,\n                    small_kernel_size=small_kernel_size,\n                    n_channels=n_channels,\n                    n_blocks=n_blocks,\n                    scaling_factor=scaling_factor)\n    \n# defining the optimiser of the model\noptimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()),lr=lr)\n# LR scheduler\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n# putting the model on the device\nmodel = model.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:14:47.200381Z","iopub.execute_input":"2022-04-10T12:14:47.200998Z","iopub.status.idle":"2022-04-10T12:14:47.246096Z","shell.execute_reply.started":"2022-04-10T12:14:47.200963Z","shell.execute_reply":"2022-04-10T12:14:47.245300Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# perceptual loss\nclass SRResNetPerceptualLoss(nn.Module):\n    def __init__(self, SRResNet, resize = True):\n        super(SRResNetPerceptualLoss, self).__init__()\n        \n        blocks = []\n        blocks.append(SRResNet.residual_blocks[:4].eval())\n        blocks.append(SRResNet.residual_blocks[4:9].eval())\n        blocks.append(SRResNet.residual_blocks[9:15].eval())\n                \n        for block in blocks:\n            for param in block.parameters():\n                param.requires_grad = False\n                \n        self.blocks = torch.nn.ModuleList(blocks)\n        self.transform = torch.nn.functional.interpolate\n        self.resize = resize\n        \n        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n        \n    def forward(self, inp, target, feature_layers = [0, 1, 2], style_layers = []):\n        if(inp.shape[1] != 3):\n            inp = inp.repeat(1, 3, 1, 1)\n            target = target.repeat(1, 3, 1, 1)\n        if(self.resize):\n            inp = self.transform(inp, mode = 'bilinear', size = (64, 64), align_corners = False)\n            target = self.transform(target, mode = 'bilinear', size = (64, 64), align_corners = False)\n        inp = inp.reshape((64,64,3,2))\n        target = target.reshape((64,64,3,2))\n\n        loss = 0.0\n        x = inp\n        y = target\n\n        for i, block in enumerate(self.blocks):\n            x = block(x)\n            y = block(y)\n                \n            if(i in feature_layers):\n                loss += torch.nn.functional.l1_loss(x, y)\n        return loss\n                \n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:14:49.035840Z","iopub.execute_input":"2022-04-10T12:14:49.036685Z","iopub.status.idle":"2022-04-10T12:14:49.050987Z","shell.execute_reply.started":"2022-04-10T12:14:49.036629Z","shell.execute_reply":"2022-04-10T12:14:49.050179Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"summary(model, (3, 24, 24))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:14:50.560162Z","iopub.execute_input":"2022-04-10T12:14:50.560443Z","iopub.status.idle":"2022-04-10T12:14:50.607122Z","shell.execute_reply.started":"2022-04-10T12:14:50.560382Z","shell.execute_reply":"2022-04-10T12:14:50.606409Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# MSE loss is the loss fucntion that needed to be optimized.\ncriterion = nn.MSELoss().to(device)\n# perceptual loss\ncriterion = SRResNetPerceptualLoss(model).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:14:51.840608Z","iopub.execute_input":"2022-04-10T12:14:51.840890Z","iopub.status.idle":"2022-04-10T12:14:51.851777Z","shell.execute_reply.started":"2022-04-10T12:14:51.840861Z","shell.execute_reply":"2022-04-10T12:14:51.850885Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# \ntrain_dataset = SRDataset(train_path, crop_size, scaling_factor)\ntest_dataset = SRDataset(test_path, crop_size, scaling_factor)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=workers,\n    pin_memory=True) \n\ntest_loader = torch.utils.data.DataLoader(test_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=workers,\n    pin_memory=True)\n\n\nfor epoch in range(start_epoch, epochs+1):\n\n    model.train()  \n    train_loss=torch.zeros(1)\n    n_iter = len(train_loader)\n\n    for i, (lr_imgs, hr_imgs) in enumerate(train_loader):\n        #with torch.no_grad():\n        if True:\n            lr_imgs = lr_imgs.to(device)\n            hr_imgs = hr_imgs.to(device)\n            #print(lr_imgs.shape)\n            sr_imgs = model(lr_imgs)\n            loss = criterion(sr_imgs, hr_imgs)  \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss+=loss.item()\n        torch.cuda.empty_cache()\n\n    epoch_loss_train=train_loss / n_iter\n\n    \n    print(f\"Epoch {epoch}. Training loss: {epoch_loss_train}\")\n\n\n\n    model.eval()  \n    test_loss=0\n    all_psnr = 0\n    n_iter = len(test_loader)\n    \n    for i, (lr_imgs, hr_imgs) in enumerate(test_loader):\n        with torch.no_grad():\n            lr_imgs = lr_imgs.to(device)\n            hr_imgs = hr_imgs.to(device)\n\n            sr_imgs = model(lr_imgs)\n            loss = criterion(sr_imgs, hr_imgs)\n\n            psnr = 10 * log10(1 / loss.item())\n            all_psnr+=psnr\n            test_loss+=loss.item()\n        \n    \n    epoch_loss_test=test_loss/n_iter\n    epoch_psnr=all_psnr / n_iter\n\n\n    print(f\"Epoch {epoch}. Testing loss: {epoch_loss_test}\")\n    print(f\"Average PSNR: {epoch_psnr} dB.\")\n\n    if epoch_psnr>pre_psnr:\n        torch.save(model, checkpoint)\n        pre_psnr=epoch_psnr\n        print('Model updated successfully')\n\n    scheduler.step()\n    print('Current Learning Rate is ï¼š',end=' ')\n    print(optimizer.state_dict()['param_groups'][0]['lr'])\n\n    print('*'*50)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:14:53.127041Z","iopub.execute_input":"2022-04-10T12:14:53.127333Z","iopub.status.idle":"2022-04-10T14:07:13.596632Z","shell.execute_reply.started":"2022-04-10T12:14:53.127302Z","shell.execute_reply":"2022-04-10T14:07:13.595704Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:11:59.165426Z","iopub.execute_input":"2022-04-10T12:11:59.165944Z","iopub.status.idle":"2022-04-10T12:11:59.185885Z","shell.execute_reply.started":"2022-04-10T12:11:59.165900Z","shell.execute_reply":"2022-04-10T12:11:59.185053Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = os.listdir('../input/div2k/DIV2K_valid_HR/DIV2K_valid_HR')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(a)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}